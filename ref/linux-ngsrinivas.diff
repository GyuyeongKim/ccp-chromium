diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6061963..29f68b0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -893,6 +893,8 @@ struct rate_sample {
 	u32  prior_delivered;	/* tp->delivered at "prior_mstamp" */
 	s32  delivered;		/* number of packets delivered over interval */
 	long interval_us;	/* time for tp->delivered to incr "delivered" */
+  long snd_int_us; /* snd interval for delivered packets */
+  long rcv_int_us; /* rcv interval for delivered packets */
 	long rtt_us;		/* RTT of last (S)ACKed packet (or -1) */
 	int  losses;		/* number of packets marked lost upon ACK */
 	u32  acked_sacked;	/* number of packets newly (S)ACKed upon ACK */
diff --git a/net/ipv4/Kconfig b/net/ipv4/Kconfig
index 6e7baaf..a1500ff 100644
--- a/net/ipv4/Kconfig
+++ b/net/ipv4/Kconfig
@@ -540,6 +540,41 @@ config TCP_CONG_VEGAS
 	window. TCP Vegas should provide less packet loss, but it is
 	not as aggressive as TCP Reno.
 
+config TCP_CONG_TESTRENO
+       tristate "TCP TestReno"
+       default n
+       ---help---
+       TCP TestReno is a new test TCP module for experimenting with programmable
+       congestion control in the linux kernel.
+
+config TCP_CONG_TESTVEGAS
+       tristate "TCP TestVegas"
+       default n
+       ---help---
+       TCP TestVegas is a new test TCP module for experimenting with
+       programmable congestion control in the linux kernel.
+
+config TCP_CONG_TESTCUBIC
+       tristate "TCP TestCubic"
+       default n
+       ---help---
+       TCP TestCubic is a new test TCP module for experimenting with
+       programmable congestion control in the linux kernel.
+
+config TCP_CONG_TESTRATE
+       tristate "TCP TestRate"
+       default n
+       ---help---
+       TCP TestRate is a new test TCP module for experimenting with
+       programmable congestion control in the linux kernel.
+
+config TCP_CONG_NIMBUS
+       tristate "TCP Nimbus"
+       default n
+       ---help---
+       TCP Nimbus is a new test TCP module for experimenting with
+       programmable congestion control in the linux kernel.
+
 config TCP_CONG_NV
        tristate "TCP NV"
        default n
diff --git a/net/ipv4/Makefile b/net/ipv4/Makefile
index 48af58a..4c64f48 100644
--- a/net/ipv4/Makefile
+++ b/net/ipv4/Makefile
@@ -43,6 +43,11 @@ obj-$(CONFIG_INET_UDP_DIAG) += udp_diag.o
 obj-$(CONFIG_INET_RAW_DIAG) += raw_diag.o
 obj-$(CONFIG_NET_TCPPROBE) += tcp_probe.o
 obj-$(CONFIG_TCP_CONG_BBR) += tcp_bbr.o
+obj-$(CONFIG_TCP_CONG_TESTRENO) += tcp_testreno.o
+obj-$(CONFIG_TCP_CONG_TESTVEGAS) += tcp_testvegas.o
+obj-$(CONFIG_TCP_CONG_TESTCUBIC) += tcp_testcubic.o
+obj-$(CONFIG_TCP_CONG_TESTRATE)  += tcp_testrate.o
+obj-$(CONFIG_TCP_CONG_NIMBUS)    += tcp_nimbus.o
 obj-$(CONFIG_TCP_CONG_BIC) += tcp_bic.o
 obj-$(CONFIG_TCP_CONG_CDG) += tcp_cdg.o
 obj-$(CONFIG_TCP_CONG_CUBIC) += tcp_cubic.o
diff --git a/net/ipv4/tcp_nimbus.c b/net/ipv4/tcp_nimbus.c
new file mode 100644
index 0000000..dc5f142
--- /dev/null
+++ b/net/ipv4/tcp_nimbus.c
@@ -0,0 +1,358 @@
+#include <linux/module.h>
+#include <net/tcp.h>
+#include <linux/math64.h>
+
+#define MTU (1500)
+#define S_TO_US (1000000)
+#define BW_ERROR_PERC_THRESH 5
+#define EWMA_SAMPLE_WT 4
+/* TODO: Hard-coding the number of bytes in the MTU is really hacky. Will fix
+   this once I figure out the right way. */
+
+#define MYRATE 60000000
+/* Rate above is in bytes per second. 1 MSS/millisecond is 12 Mbit/s or
+   1.5 MBytes/second. */
+
+/* Link capacity in bytes per second. */
+#define LINK_CAP 96200000
+/* Nimbus parameters. */
+#define NIMBUS_THRESH_DEL 15
+#define NIMBUS_ALPHA 8
+#define NIMBUS_BETA 5
+#define NIMBUS_MAX_RATE (12 * LINK_CAP / 10)
+#define NIMBUS_FRAC_DR 10
+#define NIMBUS_CROSS_TRAFFIC_EST_VALID_THRESH 13
+#define NIMBUS_EPOCH_MS 20
+#define NIMBUS_RATE_STALE_TIMEOUT_MS 100
+#define NIMBUS_MIN_SEGS_IN_FLIGHT 3
+#define NIMBUS_EWMA_RECENCY 6
+/* Error accounting parameters. */
+#define DELAY_ERROR_THRESH_US 5000
+
+struct nimbus {
+  u32 rate;           /* rate to pace packets, in bytes per second */
+  u32 min_rtt_us;     /* maintain min rtt samples */
+  u32 min_rtt_stamp;  /* time when min rtt was recorded */
+  u32 new_min_rtt;    /* new estimate of min rtt */
+  u32 last_rtt_us;    /* maintain the last rtt sample */
+  u32 ewma_rtt_us;    /* maintain an ewma of instantaneous rtt samples */
+  u32 rate_stamp;     /* last time when rate was updated */
+  u32 ewma_rtt_ovr_error; /* account rtt samples that overshoot target rtt */
+  u32 ewma_rtt_unr_error; /* account rtt samples that undershoot target rtt */
+  u32 total_samples;  /* total number of rtt samples */
+};
+
+bool min_rtt_time_to_update(struct nimbus *ca)
+{
+  return after(tcp_time_stamp, ca->min_rtt_stamp +
+               msecs_to_jiffies(ca->new_min_rtt));
+}
+
+void tcp_nimbus_pkts_acked(struct sock *sk, const struct ack_sample *sample)
+{
+  struct nimbus *ca = inet_csk_ca(sk);
+  s32 sampleRTT = sample->rtt_us;
+
+  /* Check the validity of the RTT samples */
+  if (sampleRTT <= 0) {
+    pr_info("Nimbus: unexpected sample rtt %d in pkts_acked\n", sampleRTT);
+    return;
+  }
+
+  /* Always update latest estimate of min RTT. This estimate only holds the
+   * minimum over a short period of time, namely 10 RTTs. */
+  ca->new_min_rtt = min(ca->new_min_rtt, (u32)sampleRTT);
+  /* If a sufficient period of time has elapsed since the last update to
+   * min_rtt_us, update it. */
+  if (min_rtt_time_to_update(ca)) {
+    ca->min_rtt_us = ca->new_min_rtt;
+    ca->min_rtt_stamp = tcp_time_stamp;
+    ca->new_min_rtt = 0x7fffffff;
+  }
+  if (sampleRTT > 2 * ca->last_rtt_us)
+    pr_info("Nimbus: unexpected spike in sample RTT! old: %d curr: %d\n",
+            ca->last_rtt_us,
+            sampleRTT);
+  ca->last_rtt_us = sampleRTT;
+  ca->ewma_rtt_us = ((sampleRTT * NIMBUS_EWMA_RECENCY) +
+                     (ca->ewma_rtt_us *
+                      (NIMBUS_FRAC_DR-NIMBUS_EWMA_RECENCY))) / NIMBUS_FRAC_DR;
+}
+
+static void tcp_nimbus_init(struct sock *sk)
+{
+  struct nimbus *ca = inet_csk_ca(sk);
+  pr_info("Nimbus: initializing connection\n");
+  ca->rate = MYRATE;
+  ca->min_rtt_us = 0x7fffffff;
+  ca->min_rtt_stamp = 0;
+  ca->new_min_rtt = 0x7fffffff;
+  ca->last_rtt_us = 0x7fffffff;
+  ca->ewma_rtt_us = 0x7fffffff;
+  ca->rate_stamp = 0;
+  sk->sk_max_pacing_rate = LINK_CAP;
+  sk->sk_pacing_rate = 0;
+  sk->sk_pacing_rate = ca->rate;
+  ca->ewma_rtt_ovr_error = 0;
+  ca->ewma_rtt_unr_error = 0;
+  ca->total_samples = 0;
+}
+
+static void tcp_nimbus_set_pacing_rate(struct sock *sk)
+{
+  struct tcp_sock *tp = tcp_sk(sk);
+  struct nimbus *ca = inet_csk_ca(sk);
+  u64 segs_in_flight; /* desired cwnd as rate * rtt */
+  sk->sk_pacing_rate = ca->rate;
+  if (likely (ca->ewma_rtt_us > 0)) {
+    segs_in_flight = (u64)ca->rate * ca->ewma_rtt_us;
+    do_div(segs_in_flight, MTU);
+    do_div(segs_in_flight, S_TO_US);
+    /* Add few more segments to segs_to_flight to prevent rate underflow due to
+       temporary RTT fluctuations. */
+    tp->snd_cwnd = segs_in_flight + 3;
+  }
+}
+
+static u32 estimate_cross_traffic(u32 est_bandwidth,
+                                  u32 rint,
+                                  u32 routt,
+                                  u32 rtt,
+                                  u32 min_rtt)
+{
+  s64 zt;
+  zt = (u64)est_bandwidth * rint;
+  if (routt == 0) {
+    pr_info("Nimbus: Unexpected condition rout===0 in "
+            "estimate_cross_traffic\n");
+    return 0;
+  }
+  do_div(zt, routt);
+  zt -= rint;
+  pr_info("Nimbus: Estimated cross traffic: %lld bps\n", zt);
+  if (rtt < (NIMBUS_CROSS_TRAFFIC_EST_VALID_THRESH * min_rtt / NIMBUS_FRAC_DR))
+    zt = 0;
+  else if (zt < 0)
+    zt = 0;
+  else if (zt > LINK_CAP)
+    zt = LINK_CAP;
+  pr_info("Nimbus: Corrected estimated cross traffic: %lld Mbit/s\n", zt / 125000);
+  return (u32)zt;
+}
+
+static u32 single_seg_bps(u32 rtt_us) {
+  if (rtt_us == 0) {
+    pr_info("Nimbus: unexpected rtt==0 in single_seg_bps!\n");
+    return MTU * S_TO_US / 20000; /* assume default rtt of 20ms */
+  }
+  return MTU * S_TO_US / rtt_us;
+}
+
+/* Sets rin(t+1) given rin(t) and several other network observations. Closely
+ * follows the control law from the nimbus userspace implementation. */
+static u32 nimbus_rate_control(const struct sock *sk,
+                               u32 rint,
+                               u32 routt,
+                               u32 est_bandwidth,
+                               u32 zt)
+{
+  s32 new_rate;
+  u32 rtt; /* rtt used to compute new rate */
+  u32 last_rtt;
+  u32 min_rtt;
+  u16 sign;
+  s32 delay_diff;
+  u32 expected_rtt;
+  s64 delay_term;
+  s32 spare_cap;
+  s32 rate_term;
+  u32 min_seg_rate;
+
+  struct nimbus *ca = inet_csk_ca(sk);
+  rtt = ca->ewma_rtt_us;
+  min_rtt  = ca->min_rtt_us;
+  last_rtt = ca->last_rtt_us;
+  spare_cap = (s32)est_bandwidth - zt - rint;
+  rate_term = NIMBUS_ALPHA * spare_cap / NIMBUS_FRAC_DR;
+  expected_rtt = (NIMBUS_THRESH_DEL * min_rtt)/NIMBUS_FRAC_DR;
+  delay_diff = (s32)rtt - (s32)expected_rtt;
+  delay_term = (s64)est_bandwidth * delay_diff;
+  sign = 0;
+  if (delay_term < 0) {
+    sign = 1;
+    delay_term = -delay_term;
+  }
+  if (min_rtt == 0) {
+    pr_info("Nimbus: unexpected min_rtt == 0 in rate control!\n");
+    return rint;
+  }
+  do_div(delay_term, (s64)min_rtt);
+  delay_term = delay_term * (s64)NIMBUS_BETA;
+  do_div(delay_term, (s64)NIMBUS_FRAC_DR);
+  if (sign == 1)
+    delay_term = -delay_term;
+
+  /* Error accounting */
+  ca->total_samples++;
+  if ((sign == 0) && (delay_diff > (s32)DELAY_ERROR_THRESH_US))
+    ca->ewma_rtt_ovr_error++;
+  else if ((sign == 1) && (delay_diff < -(s32)DELAY_ERROR_THRESH_US))
+    ca->ewma_rtt_unr_error++;
+
+  /* Compute new rate as a combination of delay mismatch and rate mismatch. */
+  new_rate = rint + rate_term - delay_term;
+  pr_info("Nimbus: min_rtt %d ewma_rtt %d last_rtt %d expected_rtt %d\n",
+          min_rtt, rtt, last_rtt, expected_rtt);
+  pr_info("Nimbus: rint %d Mbit/s "
+          "spare_cap %d Mbit/s rate_term %d Mbit/s "
+          "delay_diff %d us delay_term %lld "
+          "new_rate %d Mbit/s\n", 
+          rint / 125000,
+          spare_cap / 125000,
+          rate_term / 125000,
+          delay_diff,
+          delay_term,
+          new_rate / 125000);
+  /* Clamp the rate between two reasonable limits. */
+  min_seg_rate = NIMBUS_MIN_SEGS_IN_FLIGHT * single_seg_bps(rtt);
+  if (new_rate < (s32)min_seg_rate) new_rate = min_seg_rate;
+  if (new_rate > (s32)NIMBUS_MAX_RATE) new_rate = NIMBUS_MAX_RATE;
+  pr_info("Nimbus: clamped rate %d Mbit/s (%d bps)\n", new_rate / 125000, new_rate);
+  pr_info("Nimbus: total rtt samples %d overshoots %d undershoots %d\n",
+          ca->total_samples,
+          ca->ewma_rtt_ovr_error,
+          ca->ewma_rtt_unr_error);
+  return (u32)new_rate;
+}
+
+static int rate_sample_valid(const struct rate_sample *rs)
+{
+  int ret = 0;
+  if ((rs->delivered > 0) && (rs->snd_int_us > 0) && (rs->rcv_int_us > 0))
+    return 0;
+  if (rs->delivered <= 0)
+    ret |= 1;
+  if (rs->snd_int_us <= 0)
+    ret |= 2;
+  if (rs->rcv_int_us <= 0)
+    ret |= 4;
+  return ret;
+}
+
+void tcp_nimbus_check_rate_mismatch(u64 achieved_snd_rate,
+                                      u64 achieved_rcv_rate,
+                                      u32 set_rate,
+                                      const struct rate_sample *rs,
+                                      u32 perc_thresh)
+{
+  u32 diff_rate;
+  if (set_rate > achieved_snd_rate)
+    diff_rate = set_rate - achieved_snd_rate;
+  else
+    diff_rate = achieved_snd_rate - set_rate;
+  if (diff_rate > (perc_thresh * (set_rate / 100))) {
+    pr_info("Nimbus: tcp_nimbus found a rate mismatch %d Mbit/s over %ld us\n",
+            diff_rate / 125000, rs->interval_us);
+    pr_info("Nimbus: (delivered %d segments) expected: %d Mbit/s achieved: snd %lld rcv %lld\n",
+            rs->delivered,
+            set_rate / 125000,
+            achieved_snd_rate / 125000,
+            achieved_rcv_rate / 125000);
+  }
+}
+
+bool epoch_elapsed(struct nimbus *ca)
+{
+  return ((ca->rate_stamp == 0) ||
+          (after(tcp_time_stamp,
+                 ca->rate_stamp + msecs_to_jiffies(NIMBUS_EPOCH_MS))));
+}
+
+void rate_not_changed_awhile(struct nimbus *ca,
+                             int measured_valid_rate,
+                             struct tcp_sock *tp)
+{
+  if(after(tcp_time_stamp, ca->rate_stamp +
+           msecs_to_jiffies(NIMBUS_RATE_STALE_TIMEOUT_MS))) {
+    pr_info("Nimbus: Rate hasn't changed in a while! Valid rate: %d %d\n",
+            measured_valid_rate, tp->snd_cwnd);
+  }
+}
+
+void tcp_nimbus_cong_control(struct sock *sk, const struct rate_sample *rs)
+{
+  u64 snd_bw_bps;   /* send bandwidth in bytes per second */
+  u64 rcv_bw_bps;   /* recv bandwidth in bytes per second */
+  u32 zt; /* Estimate of cross traffic */
+  u32 new_rate; /* New rate proposed by nimbus */
+  int measured_valid_rate;
+
+  struct tcp_sock *tp = tcp_sk(sk);
+  struct nimbus *ca = inet_csk_ca(sk);
+  measured_valid_rate = rate_sample_valid(rs);
+  if (epoch_elapsed(ca) && measured_valid_rate == 0) {
+    pr_info("Nimbus: ======= epoch elapsed; recomputing rate ======\n");
+    /* update per-ack state */
+    ca->rate_stamp = tcp_time_stamp;
+    rcv_bw_bps = snd_bw_bps = (u64)rs->delivered * MTU * S_TO_US;
+    do_div(snd_bw_bps, rs->snd_int_us);
+    do_div(rcv_bw_bps, rs->rcv_int_us);
+    /* Check if the send or recv rates are noisily enormous. This tends to
+       happen if very few segments were delivered. Typically the tx rate seems
+       to get bumped up significantly. */
+    if ((snd_bw_bps > NIMBUS_MAX_RATE) || (rcv_bw_bps > NIMBUS_MAX_RATE)) {
+      pr_info("Nimbus: invalid snd or rcv rates %lld %lld (delivered %d) \n",
+              snd_bw_bps, rcv_bw_bps, rs->delivered);
+      rate_not_changed_awhile(ca, measured_valid_rate, tp);
+      return;
+    }
+    /* Check rate mismatch through a threshold difference between the set and
+       achieved send rates. */
+    tcp_nimbus_check_rate_mismatch(snd_bw_bps,
+                                   rcv_bw_bps,
+                                   ca->rate,
+                                   rs,
+                                   BW_ERROR_PERC_THRESH);
+    /* Perform nimbus rate control */
+    zt = estimate_cross_traffic(LINK_CAP, snd_bw_bps, rcv_bw_bps,
+                                ca->ewma_rtt_us, ca->min_rtt_us);
+    new_rate = nimbus_rate_control(sk, snd_bw_bps, rcv_bw_bps, LINK_CAP, zt);
+    /* Set the socket rate to nimbus proposed rate */
+    ca->rate = new_rate;
+    pr_info("Nimbus: Setting new rate %d Mbit/s (%d bps)\n", ca->rate / 125000, ca->rate);
+    tcp_nimbus_set_pacing_rate(sk);
+  } else
+    rate_not_changed_awhile(ca, measured_valid_rate, tp);
+}
+
+static struct tcp_congestion_ops tcp_nimbus = {
+  .init = tcp_nimbus_init,
+  .ssthresh = tcp_reno_ssthresh,
+  .pkts_acked = tcp_nimbus_pkts_acked,
+  .cong_control = tcp_nimbus_cong_control,
+  .undo_cwnd = tcp_reno_undo_cwnd,
+
+  .owner = THIS_MODULE,
+  .name  = "nimbus",
+};
+
+static int __init tcp_nimbus_register(void)
+{
+  printk(KERN_INFO "Initializing nimbus\n");
+  BUILD_BUG_ON(sizeof(struct nimbus) > ICSK_CA_PRIV_SIZE);
+  tcp_register_congestion_control(&tcp_nimbus);
+  return 0;
+}
+
+static void __exit tcp_nimbus_unregister(void)
+{
+  printk(KERN_INFO "Exiting nimbus\n");
+  tcp_unregister_congestion_control(&tcp_nimbus);
+}
+
+module_init(tcp_nimbus_register);
+module_exit(tcp_nimbus_unregister);
+
+MODULE_AUTHOR("Srinivas Narayana");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("TCP Nimbus");
diff --git a/net/ipv4/tcp_rate.c b/net/ipv4/tcp_rate.c
index 9be1581..26690d1 100644
--- a/net/ipv4/tcp_rate.c
+++ b/net/ipv4/tcp_rate.c
@@ -140,6 +140,9 @@ void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
 	snd_us = rs->interval_us;				/* send phase */
 	ack_us = skb_mstamp_us_delta(now, &rs->prior_mstamp);	/* ack phase */
 	rs->interval_us = max(snd_us, ack_us);
+  /* Record both segment send and ack receive intervals. */
+  rs->snd_int_us = snd_us;
+  rs->rcv_int_us = ack_us;
 
 	/* Normally we expect interval_us >= min-rtt.
 	 * Note that rate may still be over-estimated when a spuriously
diff --git a/net/ipv4/tcp_testcubic.c b/net/ipv4/tcp_testcubic.c
new file mode 100644
index 0000000..dfe0457
--- /dev/null
+++ b/net/ipv4/tcp_testcubic.c
@@ -0,0 +1,260 @@
+#include <linux/module.h>
+#include <net/tcp.h>
+#include <linux/math64.h>
+
+#define BICTCP_BETA_SCALE    1024	/* Scale factor beta calculation
+					 * max_cwnd = snd_cwnd * beta
+					 */
+#define	BICTCP_HZ		10	/* BIC HZ 2^10 = 1024 */
+static int beta __read_mostly = 717;	/* = 717/1024 (BICTCP_BETA_SCALE) */
+static int bic_scale __read_mostly = 41;
+static u32 cube_rtt_scale __read_mostly;
+static u64 cube_factor __read_mostly;
+
+module_param(beta, int, 0644);
+MODULE_PARM_DESC(beta, "beta for multiplicative increase");
+module_param(bic_scale, int, 0444);
+MODULE_PARM_DESC(bic_scale, "scale (scaled by 1024) value for bic function"
+                 " (bic_scale/1024)"); 
+
+struct testcubic {
+  u32 bic_K;         /* origin point on time axis */
+  u32 origin_cwnd;   /* origin point on cwnd axis */
+  u32 epoch_start;   /* time-axis adjustment for cubic function */
+  u32 last_time;     /* last time when a cubic update happened */
+  u32 last_cwnd;     /* last cwnd value when a cubic update happened */
+  u32 last_max_cwnd; /* the max window from the last cubic 'turnaround'.
+                        Useful to compute the ssthresh and origin_cwnd next
+                        time. */
+  u32 cnt;           /* cwnd/(additive increase to the congestion window) */
+};
+
+/* calculate the cubic root of x using a table lookup followed by one
+ * Newton-Raphson iteration.
+ * Avg err ~= 0.195%
+ */
+static u32 cubic_root(u64 a)
+{
+	u32 x, b, shift;
+	/*
+	 * cbrt(x) MSB values for x MSB values in [0..63].
+	 * Precomputed then refined by hand - Willy Tarreau
+	 *
+	 * For x in [0..63],
+	 *   v = cbrt(x << 18) - 1
+	 *   cbrt(x) = (v[x] + 10) >> 6
+	 */
+	static const u8 v[] = {
+		/* 0x00 */    0,   54,   54,   54,  118,  118,  118,  118,
+		/* 0x08 */  123,  129,  134,  138,  143,  147,  151,  156,
+		/* 0x10 */  157,  161,  164,  168,  170,  173,  176,  179,
+		/* 0x18 */  181,  185,  187,  190,  192,  194,  197,  199,
+		/* 0x20 */  200,  202,  204,  206,  209,  211,  213,  215,
+		/* 0x28 */  217,  219,  221,  222,  224,  225,  227,  229,
+		/* 0x30 */  231,  232,  234,  236,  237,  239,  240,  242,
+		/* 0x38 */  244,  245,  246,  248,  250,  251,  252,  254,
+	};
+
+	b = fls64(a);
+	if (b < 7) {
+		/* a in [0..63] */
+		return ((u32)v[(u32)a] + 35) >> 6;
+	}
+
+	b = ((b * 84) >> 8) - 1;
+	shift = (a >> (b * 3));
+
+	x = ((u32)(((u32)v[shift] + 10) << b)) >> 6;
+
+	/*
+	 * Newton-Raphson iteration
+	 *                         2
+	 * x    = ( 2 * x  +  a / x  ) / 3
+	 *  k+1          k         k
+	 */
+	x = (2 * x + (u32)div64_u64(a, (u64)x * (u64)(x - 1)));
+	x = ((x * 341) >> 10);
+	return x;
+}
+
+static void tcp_testcubic_cong_avoid(struct sock *sk, u32 ack, u32 acked)
+{
+  u32 bic_target, delta, cwnd;
+  u64 offs, t;
+
+  struct tcp_sock *tp = tcp_sk(sk);
+  struct testcubic *ca = inet_csk_ca(sk);
+
+  /* Standard boilerplate for cong_avoid functions */
+  if (!tcp_is_cwnd_limited(sk))
+    return;
+
+  if (tcp_in_slow_start(tp)) {
+    acked = tcp_slow_start(tp, acked);
+    if (! acked)
+      return;
+  }
+
+  /* Begin actual cubic update logic. */
+  /* Only update windows if something changed in the last jiffy. */
+  cwnd = tp->snd_cwnd;
+  if (ca->last_cwnd == cwnd &&
+	    (s32)(tcp_time_stamp - ca->last_time) <= HZ / 32)
+		return;
+
+  /* Save a few things before starting cwnd updates. */
+  ca->last_cwnd = cwnd;
+  ca->last_time = tcp_time_stamp;
+
+  /* Shamelessly copied cubic function arithmetic from cubic tcp
+     implementation. */
+	if (ca->epoch_start == 0) {
+		ca->epoch_start = tcp_time_stamp;	/* record beginning */
+
+		if (ca->last_max_cwnd <= cwnd) {
+			ca->bic_K = 0;
+			ca->origin_cwnd = cwnd;
+		} else {
+			/* Compute new K based on
+			 * (wmax-cwnd) * (srtt>>3 / HZ) / c * 2^(3*bictcp_HZ)
+			 */
+			ca->bic_K = cubic_root(cube_factor
+					       * (ca->last_max_cwnd - cwnd));
+			ca->origin_cwnd = ca->last_max_cwnd;
+		}
+	}
+
+  /* Here come the actual cubic updates with gory operations to avoid
+   * overflows. */
+  t = (s32)(tcp_time_stamp - ca->epoch_start);
+  /* ignoring adding the min_delay to t */
+	/* change the unit from HZ to bictcp_HZ */
+	t <<= BICTCP_HZ;
+	do_div(t, HZ);
+
+	if (t < ca->bic_K)		/* t - K */
+		offs = ca->bic_K - t;
+	else
+		offs = t - ca->bic_K;
+
+	/* c/rtt * (t-K)^3 */
+	delta = (cube_rtt_scale * offs * offs * offs) >> (10+3*BICTCP_HZ);
+	if (t < ca->bic_K)                            /* below origin*/
+		bic_target = ca->origin_cwnd - delta;
+	else                                          /* above origin*/
+		bic_target = ca->origin_cwnd + delta;
+
+	/* cubic function - calc bictcp_cnt*/
+	if (bic_target > cwnd) {
+		ca->cnt = cwnd / (bic_target - cwnd);
+	} else {
+		ca->cnt = 100 * cwnd;              /* very small increment*/
+	}
+}
+EXPORT_SYMBOL_GPL(tcp_testcubic_cong_avoid);
+
+static u32 tcp_testcubic_ssthresh(struct sock *sk)
+{
+  /* I wonder if there's an implicit assumption that this function is called
+   * whenever there is a loss... */
+  const struct tcp_sock *tp = tcp_sk(sk);
+  struct testcubic *ca = inet_csk_ca(sk);
+
+  /* Set the last max cwnd to current cwnd. */
+  ca->last_max_cwnd = tp->snd_cwnd;
+  return max((tp->snd_cwnd * beta) / BICTCP_BETA_SCALE, 2U);
+}
+EXPORT_SYMBOL_GPL(tcp_testcubic_ssthresh);
+
+static u32 tcp_testcubic_undo_cwnd(struct sock *sk)
+{
+  /* Still have to understand crystal-clearly where and how undo_cwnd is
+   * used. */
+  struct tcp_sock *tp = tcp_sk(sk);
+  struct testcubic *ca = inet_csk_ca(sk);
+
+  return max(tp->snd_cwnd, ca->last_max_cwnd);
+}
+EXPORT_SYMBOL_GPL(tcp_testcubic_undo_cwnd);
+
+/* This function may not even be needed for the preliminary implementation... */
+/* static void tcp_testcubic_cwnd_event(struct sock *sk, enum tcp_ca_event event) */
+/* { */
+/*   struct tcp_sock *tp = tcp_sk(sk); */
+/*   struct testcubic *ca = inet_csk_ca(sk); */
+
+/*   if (event == CA_EVENT_TX_START) { */
+/*     /\* Restarting transmission after an idle period... *\/ */
+/*     u32 now = tcp_time_stamp; */
+/*     s32 delta; */
+
+/*     delta = now - tp->lsndtime; */
+/*     if (ca->epoch_start && delta >= 0) { */
+/*       ca->epoch_start += delta; /\* adjust the cubic curve as though it's beginning */
+/*                                    from some time in the recent past, instead of a */
+/*                                    long time ago...*\/ */
+/*       if (after(ca->epoch_start, now)) */
+/*         ca->epoch_start = now; */
+/*     } */
+/*     return; */
+/*   } */
+/* } */
+
+static void tcp_testcubic_init(struct sock *sk)
+{
+  struct testcubic *testcubic = inet_csk_ca(sk);
+  testcubic->bic_K = 0;
+  testcubic->origin_cwnd = 0;
+  testcubic->epoch_start = 0;
+  testcubic->last_max_cwnd = 0;
+  testcubic->last_time = 0;
+  testcubic->last_cwnd = 0;
+}
+EXPORT_SYMBOL_GPL(tcp_testcubic_init);
+
+static void tcp_testcubic_set_state(struct sock *sk, u8 new_state)
+{
+  if (new_state == TCP_CA_Loss) {
+    tcp_testcubic_init(sk);
+  }
+}
+EXPORT_SYMBOL_GPL(tcp_testcubic_set_state);
+
+static struct tcp_congestion_ops tcp_testcubic = {
+  .cong_avoid = tcp_testcubic_cong_avoid,
+  .ssthresh = tcp_testcubic_ssthresh,
+  .undo_cwnd = tcp_testcubic_undo_cwnd,
+  .set_state = tcp_testcubic_set_state,
+  .init = tcp_testcubic_init,
+  /*  .cwnd_event = tcp_testcubic_cwnd_event, */
+
+  .owner = THIS_MODULE,
+  .name = "testcubic",
+};
+
+static int __init tcp_testcubic_register(void)
+{
+  printk(KERN_INFO "Initializing testcubic\n");
+  BUILD_BUG_ON(sizeof(struct testcubic) > ICSK_CA_PRIV_SIZE);
+
+  /* Boilerplate initializations from TCP's cubic implementations. */
+	cube_rtt_scale = (bic_scale * 10);	/* 1024*c/rtt */
+  cube_factor = 1ull << (10+3*BICTCP_HZ); /* 2^40 */
+  do_div(cube_factor, bic_scale * 10);
+  
+  tcp_register_congestion_control(&tcp_testcubic);
+  return 0;
+}
+
+static void __exit tcp_testcubic_unregister(void)
+{
+  printk(KERN_INFO "Exiting testcubic\n");
+  tcp_unregister_congestion_control(&tcp_testcubic);
+}
+
+module_init(tcp_testcubic_register);
+module_exit(tcp_testcubic_unregister);
+
+MODULE_AUTHOR("Srinivas Narayana");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("TCP Test Cubic");
diff --git a/net/ipv4/tcp_testrate.c b/net/ipv4/tcp_testrate.c
new file mode 100644
index 0000000..a6041cc
--- /dev/null
+++ b/net/ipv4/tcp_testrate.c
@@ -0,0 +1,114 @@
+#include <linux/module.h>
+#include <net/tcp.h>
+#include <linux/math64.h>
+
+#define MTU (1500)
+#define S_TO_US (1000000)
+#define BW_ERROR_PERC_THRESH 15
+/* TODO: Hard-coding the number of bytes in the MTU is really hacky. Will fix
+   this once I figure out the right way. */
+
+#define MYRATE 128000000
+/* Rate above is in bytes per second. 1 MSS/millisecond is 12 Mbit/s or
+   1.5 MBytes/second. */
+
+struct testrate {
+  u32 rate;           /* rate to pace packets, in bytes per second */
+};
+
+static void tcp_testrate_init(struct sock *sk)
+{
+  struct testrate *ca = inet_csk_ca(sk);
+  ca->rate = MYRATE;
+  sk->sk_max_pacing_rate = ca->rate;
+  sk->sk_pacing_rate = 0;
+  sk->sk_pacing_rate = ca->rate;
+}
+
+static int rate_sample_valid(const struct rate_sample *rs)
+{
+  return (rs->delivered > 0) && (rs->snd_int_us > 0) && (rs->rcv_int_us > 0);
+}
+
+void tcp_testrate_check_rate_mismatch(u64 achieved_snd_rate,
+                                      u64 achieved_rcv_rate,
+                                      u32 set_rate,
+                                      const struct rate_sample *rs,
+                                      u32 perc_thresh)
+{
+  u32 diff_rate;
+  diff_rate = set_rate - achieved_snd_rate;
+  if (set_rate > achieved_snd_rate &&
+      diff_rate > (perc_thresh * (set_rate / 100))) {
+    pr_info("TestRate: found a rate mismatch %d bps over %ld us\n",
+            diff_rate, rs->interval_us);
+    pr_info("TestRate: delivered %d bytes. expected rate: %d achieved: snd %lld"
+            " rcv %lld\n",
+            rs->delivered,
+            set_rate,
+            achieved_snd_rate,
+            achieved_rcv_rate);
+  }
+}
+
+void tcp_testrate_cong_control(struct sock *sk, const struct rate_sample *rs)
+{
+  u64 snd_bw_bps;   /* send bandwidth in bytes per second */
+  u64 rcv_bw_bps;   /* recv bandwidth in bytes per second */
+  u64 segs_in_flight; /* compute desired cwnd as rate * rtt */
+
+  struct tcp_sock *tp = tcp_sk(sk);
+  struct testrate *ca = inet_csk_ca(sk);
+  if (rate_sample_valid(rs)) {
+    rcv_bw_bps = snd_bw_bps = (u64)rs->delivered * MTU * S_TO_US;
+    do_div(snd_bw_bps, rs->snd_int_us);
+    do_div(rcv_bw_bps, rs->rcv_int_us);
+    /* Check rate mismatch through a threshold difference between the set and
+       achieved send rates. */
+    tcp_testrate_check_rate_mismatch(snd_bw_bps,
+                                     rcv_bw_bps,
+                                     ca->rate,
+                                     rs,
+                                     BW_ERROR_PERC_THRESH);
+    /* Want to ensure window can support the set rate. */
+    if (likely (rs->rtt_us > 0)) {
+      segs_in_flight = (u64)ca->rate * rs->rtt_us;
+      do_div(segs_in_flight, MTU);
+      do_div(segs_in_flight, S_TO_US);
+      /* Add one more segment to segs_to_flight to prevent rate underflow due to
+         temporary RTT fluctuations. */
+      tp->snd_cwnd = segs_in_flight + 3;
+    }
+  }
+}
+
+static struct tcp_congestion_ops tcp_testrate = {
+  .init = tcp_testrate_init,
+  .ssthresh = tcp_reno_ssthresh,
+  .cong_control = tcp_testrate_cong_control,
+  .undo_cwnd = tcp_reno_undo_cwnd,
+
+  .owner = THIS_MODULE,
+  .name  = "testrate",
+};
+
+static int __init tcp_testrate_register(void)
+{
+  printk(KERN_INFO "Initializing testrate\n");
+  BUILD_BUG_ON(sizeof(struct testrate) > ICSK_CA_PRIV_SIZE);
+  tcp_register_congestion_control(&tcp_testrate);
+  return 0;
+}
+
+static void __exit tcp_testrate_unregister(void)
+{
+  printk(KERN_INFO "Exiting testrate\n");
+  tcp_unregister_congestion_control(&tcp_testrate);
+}
+
+module_init(tcp_testrate_register);
+module_exit(tcp_testrate_unregister);
+
+MODULE_AUTHOR("Srinivas Narayana");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("TCP Test Rate");
diff --git a/net/ipv4/tcp_testreno.c b/net/ipv4/tcp_testreno.c
new file mode 100644
index 0000000..af450ac
--- /dev/null
+++ b/net/ipv4/tcp_testreno.c
@@ -0,0 +1,74 @@
+/* Hello world kernel module. */
+
+#include <linux/module.h>
+#include <net/tcp.h>
+
+/* Slow start threshold is half the congestion window (min 2) */
+u32 tcp_testreno_ssthresh(struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+
+	return max(tp->snd_cwnd >> 1U, 2U);
+}
+EXPORT_SYMBOL_GPL(tcp_testreno_ssthresh);
+
+u32 tcp_testreno_undo_cwnd(struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+
+	return max(tp->snd_cwnd, tp->snd_ssthresh << 1);
+}
+EXPORT_SYMBOL_GPL(tcp_testreno_undo_cwnd);
+
+/*
+ * TCP Reno congestion control
+ * This is special case used for fallback as well.
+ */
+/* This is Jacobson's slow start and congestion avoidance.
+ * SIGCOMM '88, p. 328.
+ */
+void tcp_testreno_cong_avoid(struct sock *sk, u32 ack, u32 acked)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (!tcp_is_cwnd_limited(sk))
+		return;
+
+	/* In "safe" area, increase. */
+	if (tcp_in_slow_start(tp)) {
+		acked = tcp_slow_start(tp, acked);
+		if (!acked)
+			return;
+	}
+	/* In dangerous area, increase slowly. */
+	tcp_cong_avoid_ai(tp, tp->snd_cwnd, acked);
+}
+EXPORT_SYMBOL_GPL(tcp_testreno_cong_avoid);
+
+struct tcp_congestion_ops tcp_testreno_congestion_ops = {
+  .flags = TCP_CONG_NON_RESTRICTED,
+  .name = "testreno",
+  .owner = THIS_MODULE,
+  .ssthresh = tcp_testreno_ssthresh,
+  .cong_avoid = tcp_testreno_cong_avoid,
+  .undo_cwnd = tcp_testreno_undo_cwnd,
+};
+
+static int __init tcp_testreno_register(void)
+{
+  printk(KERN_INFO "Init testreno\n");
+  return tcp_register_congestion_control(&tcp_testreno_congestion_ops);
+}
+
+static void __exit tcp_testreno_unregister(void)
+{
+  printk(KERN_INFO "Exit testreno\n");
+  tcp_unregister_congestion_control(&tcp_testreno_congestion_ops);
+}
+
+module_init(tcp_testreno_register);
+module_exit(tcp_testreno_unregister);
+
+MODULE_AUTHOR("Srinivas Narayana <ngsrinivas@gmail.com>");
+MODULE_DESCRIPTION("Test module for experimentation");
+MODULE_LICENSE("GPL");
diff --git a/net/ipv4/tcp_testvegas.c b/net/ipv4/tcp_testvegas.c
new file mode 100644
index 0000000..f953508
--- /dev/null
+++ b/net/ipv4/tcp_testvegas.c
@@ -0,0 +1,109 @@
+#include <linux/module.h>
+#include <net/tcp.h>
+
+static int alpha = 2;
+static int beta  = 4;
+static int gamma = 1;
+
+module_param(alpha, int, 0644);
+MODULE_PARM_DESC(alpha, "lower bound of packets in network");
+module_param(beta, int, 0644);
+MODULE_PARM_DESC(beta, "upper bound of packets in network");
+module_param(gamma, int, 0644);
+MODULE_PARM_DESC(gamma, "upper bound of slow-start-excess packets");
+
+struct testvegas {
+  u32 baseRTT; /* minimum RTT across *all* samples */
+  u32 minRTT;  /* minimum RTT over per-RTT samples */
+  u32 beg_snd_una; /* left edge during last RTT */
+};
+
+/* Maintain two separate min-estimators: minRTT over the last RTT samples;
+ * baseRTT over RTT samples forever. */
+void tcp_testvegas_pkts_acked(struct sock *sk, const struct ack_sample *sample)
+{
+  struct testvegas *testvegas = inet_csk_ca(sk);
+  u32 sampleRTT = sample->rtt_us;
+
+  testvegas->minRTT  = min(sampleRTT, testvegas->minRTT);
+  testvegas->baseRTT = min(sampleRTT, testvegas->baseRTT);
+}
+EXPORT_SYMBOL_GPL(tcp_testvegas_pkts_acked);
+
+void tcp_testvegas_cong_avoid(struct sock *sk, u32 ack, u32 acked)
+{
+  struct tcp_sock *tp = tcp_sk(sk);
+  struct testvegas *testvegas = inet_csk_ca(sk);
+  u32 diff;
+
+  /* Simplified vegas control loop below; disregarding ugly details about number
+     of RTT samples, slow start, etc. */
+
+  /* detect one RTT elapsed: one window of packets acked? */
+  if (after(ack, testvegas->beg_snd_una)) {
+    diff = tp->snd_cwnd*(testvegas->minRTT-testvegas->baseRTT)/testvegas->minRTT;
+    testvegas->beg_snd_una = tp->snd_nxt;
+    if (tcp_in_slow_start(tp)) {
+      if (diff > gamma) {
+      /* Gotta set the ssthresh somewhere; else will permanently be stuck in
+         slow start. */
+        tp->snd_ssthresh = min(tp->snd_ssthresh, tp->snd_cwnd-1);
+        /* A couple of options. We could either directly jump to the target cwnd
+           and rate, or do additive decrease until the point of convergence. */
+        tp->snd_cwnd--;
+      } else {
+        tcp_slow_start(tp, acked);
+      }
+    } else if (diff < alpha) {
+      tp->snd_cwnd++;
+    } else if (diff > beta) {
+      tp->snd_cwnd--;
+    } else {
+      /* Sending just as fast as we should be. */
+    }
+    printk(KERN_INFO "testvegas base %d min %d\n", testvegas->baseRTT, testvegas->minRTT);
+    /* wipe out minRTT for next RTT */
+    testvegas->minRTT = 0x7fffffff;
+  }
+}
+
+void tcp_testvegas_init(struct sock *sk)
+{
+  struct testvegas *testvegas = inet_csk_ca(sk);
+  testvegas->baseRTT = 0x7fffffff;
+  testvegas->minRTT  = 0x7fffffff;
+  testvegas->beg_snd_una = 1;
+}
+EXPORT_SYMBOL_GPL(tcp_testvegas_init);
+
+static struct tcp_congestion_ops tcp_testvegas = {
+  .init = tcp_testvegas_init,
+  .ssthresh = tcp_reno_ssthresh,
+  .undo_cwnd = tcp_reno_undo_cwnd, /* double check what this is */
+  .cong_avoid = tcp_testvegas_cong_avoid,
+  .pkts_acked = tcp_testvegas_pkts_acked,
+
+  .owner = THIS_MODULE,
+  .name = "testvegas",
+};
+
+static int __init tcp_testvegas_register(void)
+{
+  printk(KERN_INFO "Initializing testvegas\n");
+  BUILD_BUG_ON(sizeof(struct testvegas) > ICSK_CA_PRIV_SIZE);
+  tcp_register_congestion_control(&tcp_testvegas);
+  return 0;
+}
+
+static void __exit tcp_testvegas_unregister(void)
+{
+  printk(KERN_INFO "Exiting testvegas\n");
+  tcp_unregister_congestion_control(&tcp_testvegas);
+}
+
+module_init(tcp_testvegas_register);
+module_exit(tcp_testvegas_unregister);
+
+MODULE_AUTHOR("Srinivas Narayana");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("TCP Test Vegas");
